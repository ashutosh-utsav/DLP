{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85c123f9",
   "metadata": {},
   "source": [
    "# week 1 GA solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab0d48c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b3d5a6",
   "metadata": {},
   "source": [
    "**Q1 checking all the option**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9521ad",
   "metadata": {},
   "source": [
    "checking total number of configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1696306a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['as', 'bn', 'gu', 'hi', 'kn', 'ml', 'mr', 'or', 'pa', 'ta', 'te']\n",
      "\n",
      "Total number of configs: 11\n"
     ]
    }
   ],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "from pprint import pprint  # optional, for pretty printing\n",
    "\n",
    "dataset = \"ai4bharat/naamapadam\"\n",
    "config_names = get_dataset_config_names(dataset)\n",
    "\n",
    "pprint(config_names)\n",
    "print(f\"\\nTotal number of configs: {len(config_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413ae4d9",
   "metadata": {},
   "source": [
    "Checking the Hindi subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec5e6fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 985787/985787 [00:07<00:00, 135793.72 examples/s]\n",
      "Generating test split: 100%|██████████| 867/867 [00:00<00:00, 67303.24 examples/s]\n",
      "Generating validation split: 100%|██████████| 13460/13460 [00:00<00:00, 130810.21 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples in Hindi: 985787\n"
     ]
    }
   ],
   "source": [
    "hindi = datasets.load_dataset(dataset, \"hi\")\n",
    "\n",
    "hindi_train = len(hindi[\"train\"])\n",
    "print(f\"Number of training examples in Hindi: {hindi_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd76c91e",
   "metadata": {},
   "source": [
    "checking the Tamil subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6450269a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 497882/497882 [00:02<00:00, 187380.09 examples/s]\n",
      "Generating test split: 100%|██████████| 758/758 [00:00<00:00, 55404.60 examples/s]\n",
      "Generating validation split: 100%|██████████| 2795/2795 [00:00<00:00, 121133.73 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples in Tamil: 497882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tamil = datasets.load_dataset(dataset, \"ta\")\n",
    "\n",
    "tamil_train = len(tamil[\"train\"])\n",
    "print(f\"Number of training examples in Tamil: {tamil_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dc350e",
   "metadata": {},
   "source": [
    "checking the label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "554fee94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes (labels): 7\n",
      "Label names: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n"
     ]
    }
   ],
   "source": [
    "label_names = hindi['train'].features['ner_tags'].feature.names\n",
    "print(f\"Number of classes (labels): {len(label_names)}\")\n",
    "print(\"Label names:\", label_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac33b98",
   "metadata": {},
   "source": [
    "**Q2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a8e2b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d8d8c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"ai4bharat/naamapadam\", \"ta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4dfc278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache directory: /home/ashutosh/.cache/huggingface/datasets/ai4bharat___naamapadam/ta/1.0.0/9d4f21ac57d11ed4f9ea64854fdc9f5618e61acc\n"
     ]
    }
   ],
   "source": [
    "# Get the cache directory used by the dataset\n",
    "cache_dir = ds.cache_files['train'][0]['filename']\n",
    "cache_dir = os.path.dirname(cache_dir)  # Get directory path\n",
    "\n",
    "print(\"Cache directory:\", cache_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa96aeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in cache:\n",
      "naamapadam-test.arrow\n",
      "dataset_info.json\n",
      "LICENSE\n",
      "naamapadam-validation.arrow\n",
      "naamapadam-train.arrow\n"
     ]
    }
   ],
   "source": [
    "# List all files in the cache directory\n",
    "cached_files = os.listdir(cache_dir)\n",
    "print(\"Files in cache:\")\n",
    "for file in cached_files:\n",
    "    print(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168f150b",
   "metadata": {},
   "source": [
    "**Q3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abe78e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total dataset size: 179.29 MB\n"
     ]
    }
   ],
   "source": [
    "total_bytes = sum(\n",
    "    os.path.getsize(os.path.join(cache_dir, f)) for f in os.listdir(cache_dir)\n",
    ")\n",
    "total_mb = total_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"\\nTotal dataset size: {total_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b017e09b",
   "metadata": {},
   "source": [
    "**Q4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1268d61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 497882\n"
     ]
    }
   ],
   "source": [
    "train_sample = len(ds['train'])\n",
    "print(f\"Number of training samples: {train_sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac743f",
   "metadata": {},
   "source": [
    "**Q5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "487e2223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 497882/497882 [01:07<00:00, 7362.54 examples/s]\n",
      "Map: 100%|██████████| 758/758 [00:00<00:00, 6776.80 examples/s]\n",
      "Map: 100%|██████████| 2795/2795 [00:00<00:00, 7740.19 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 6001876 (6.00 million)\n"
     ]
    }
   ],
   "source": [
    "ds5 = load_dataset(\"ai4bharat/naamapadam\", \"ta\")\n",
    "\n",
    "# Step 2: Define a function to count tokens in the 'tokens' field\n",
    "def count_tokens(example):\n",
    "    return {\"num tokens\": len(example[\"tokens\"])}\n",
    "\n",
    "# Step 3: Apply the function to each example in the dataset\n",
    "ds5 = ds5.map(count_tokens)\n",
    "\n",
    "# Step 4: Sum all tokens in 'train', 'validation', and 'test'\n",
    "total_tokens = 0\n",
    "for split in ds5:\n",
    "    total_tokens += sum(ds5[split][\"num tokens\"])\n",
    "\n",
    "# Convert to millions\n",
    "total_tokens_million = total_tokens / 1_000_000\n",
    "\n",
    "print(f\"Total number of tokens: {total_tokens} ({total_tokens_million:.2f} million)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc265d4",
   "metadata": {},
   "source": [
    "**Q7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bf02956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f41deed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 501435/501435 [01:04<00:00, 7802.37 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples in ds: 501435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ds_full = load_dataset(\"ai4bharat/naamapadam\", \"ta\")\n",
    "\n",
    "# Step 2: Concatenate splits in the order [train, test, validation]\n",
    "combined_ds = concatenate_datasets([ds_full[\"train\"], ds_full[\"test\"], ds_full[\"validation\"]])\n",
    "\n",
    "# Step 3: Add a new column \"text\" by joining tokens\n",
    "def add_text_column(example):\n",
    "    return {\"text\": \" \".join(example[\"tokens\"])}\n",
    "\n",
    "combined_ds = combined_ds.map(add_text_column)\n",
    "\n",
    "# Step 4: Remove the \"tokens\" and \"ner_tags\" columns\n",
    "combined_ds = combined_ds.remove_columns([\"tokens\", \"ner_tags\"])\n",
    "\n",
    "# Step 5: Assign to variable ds and print total number of samples\n",
    "ds = combined_ds\n",
    "print(f\"Total number of samples in ds: {len(ds)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e41c52",
   "metadata": {},
   "source": [
    "**Q9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf7e497f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 501435/501435 [00:03<00:00, 141958.96 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples after filtering: 370494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define filter function\n",
    "def has_minimum_tokens(example):\n",
    "    return len(example[\"text\"].split()) >= 6\n",
    "\n",
    "# Step 2: Apply filtering\n",
    "ds = ds.filter(has_minimum_tokens)\n",
    "\n",
    "# Step 3: Print the number of samples remaining\n",
    "print(f\"Number of samples after filtering: {len(ds)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621acf03",
   "metadata": {},
   "source": [
    "**Q10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7844bb7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8468f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".envdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
